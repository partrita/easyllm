# EasyLLM

EasyLLMμ€ μ¤ν” μ†μ¤ λ° ν΄λ΅μ¦λ“ μ†μ¤ λ€κ·λ¨ μ–Έμ–΄ λ¨λΈ(LLM) μ‘μ—…μ„ μ„ν• μ μ©ν• λ„κµ¬μ™€ λ°©λ²•μ„ μ κ³µν•λ” μ¤ν” μ†μ¤ ν”„λ΅μ νΈμ…λ‹λ‹¤.

EasyLLMμ€ OpenAIμ Completion APIμ™€ νΈν™λλ” ν΄λΌμ΄μ–ΈνΈλ¥Ό κµ¬ν„ν•©λ‹λ‹¤. μ¦‰, `openai.ChatCompletion`μ„ μλ¥Ό λ“¤μ–΄ `huggingface.ChatCompletion`μΌλ΅ μ‰½κ² λ°”κΏ€ μ μμµλ‹λ‹¤.

* [ChatCompletion ν΄λΌμ΄μ–ΈνΈ](./clients)
* [ν”„λ΅¬ν”„νΈ μ ν‹Έλ¦¬ν‹°](./prompt_utils)
* [μμ ](./examples)

## π€ μ‹μ‘ν•κΈ°

pipλ¥Ό ν†µν•΄ EasyLLMμ„ μ„¤μΉν•©λ‹λ‹¤:

```bash
pip install easyllm
```

κ·Έλ° λ‹¤μ ν΄λΌμ΄μ–ΈνΈλ¥Ό κ°€μ Έμ™€μ„ μ‚¬μ©ν•κΈ° μ‹μ‘ν•©λ‹λ‹¤:

```python

from easyllm.clients import huggingface

# llama2 ν”„λ΅¬ν”„νΈλ¥Ό λΉλ“ν•λ” ν—¬νΌ
huggingface.prompt_builder = "llama2"

response = huggingface.ChatCompletion.create(
    model="meta-llama/Llama-2-70b-chat-hf",
    messages=[
        {"role": "system", "content": "\nλ‹Ήμ‹ μ€ ν•΄μ μ²λΌ λ§ν•λ” λ„μ›€μ΄ λλ” μ΅°μμ…λ‹λ‹¤. μ•„λ¥΄!"},
        {"role": "user", "content": "νƒμ–‘μ΄λ€ λ¬΄μ—‡μΈκ°€?"},
    ],
    temperature=0.9,
    top_p=0.6,
    max_tokens=256,
)

print(response)
```
κ²°κ³Όλ” λ‹¤μκ³Ό κ°™μµλ‹λ‹¤

```bash
{
  "id": "hf-lVC2iTMkFJ",
  "object": "chat.completion",
  "created": 1690661144,
  "model": "meta-llama/Llama-2-70b-chat-hf",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": " μ•„λ¥΄λ¥΄, νƒμ–‘μ€ ν•λμ— λ–  μλ” μ»¤λ‹¤λ€ λ¶λ©μ–΄λ¦¬λΌλ„¤, μΉκµ¬! μ°λ¦¬ μ•„λ¦„λ‹¤μ΄ ν–‰μ„±μ— λΉ›κ³Ό λ”°μ¤ν•¨μ„ μ£Όλ” μ›μ²μ΄κ³ , κ°•λ ¥ν• νμ„ κ°€μ΅μ§€, μ•κ² λ‚? νƒμ–‘μ΄ μ—†λ‹¤λ©΄ μ°λ¦° μ–΄λ‘  μ†μ„ ν•­ν•΄ν•λ©° κΈΈμ„ μƒκ³  μ¶”μ„μ— λ–¨κ² λ  ν…λ‹, νƒμ–‘μ„ μ„ν•΄ νμ°¨κ² \"μ•Όλ¥΄!\"λ¥Ό μ™ΈμΉμκ³ , μΉκµ¬λ“¤! μ•„λ¥΄λ¥΄!"
      },
      "finish_reason": null
    }
  ],
  "usage": {
    "prompt_tokens": 111,
    "completion_tokens": 299,
    "total_tokens": 410
  }
}
```

λ‹¤λ¥Έ μμ λ¥Ό ν™•μΈν•μ„Έμ”:

* [μμ„Έν• ChatCompletion μμ ](examples/chat-completion-api)
* [μ±„ν… μ”μ²­ μ¤νΈλ¦¬λ° λ°©λ²• μμ ](examples/stream-chat-completion)
* [ν…μ¤νΈ μ”μ²­ μ¤νΈλ¦¬λ° λ°©λ²• μμ ](examples/stream-text-completion)
* [μμ„Έν• Completion μμ ](examples/text-completion-api)
* [μ„λ² λ”© μƒμ„±](examples/get-embeddings)


## π’π» OpenAIμ—μ„ HuggingFaceλ΅ λ§μ΄κ·Έλ μ΄μ…

OpenAIμ—μ„ HuggingFaceλ΅ λ§μ΄κ·Έλ μ΄μ…ν•λ” κ²ƒμ€ μ‰½μµλ‹λ‹¤. κ°€μ Έμ¤κΈ° λ¬Έκ³Ό μ‚¬μ©ν•λ ¤λ” ν΄λΌμ΄μ–ΈνΈλ¥Ό λ³€κ²½ν•κ³  μ„ νƒμ μΌλ΅ ν”„λ΅¬ν”„νΈ λΉλ”λ¥Ό λ³€κ²½ν•λ©΄ λ©λ‹λ‹¤.

```diff
- import openai
+ from easyllm.clients import huggingface
+ huggingface.prompt_builder = "llama2"


- response = openai.ChatCompletion.create(
+ response = huggingface.ChatCompletion.create(
-    model="gpt-3.5-turbo",
+    model="meta-llama/Llama-2-70b-chat-hf",
    messages=[
        {"role": "system", "content": "λ‹Ήμ‹ μ€ λ„μ›€μ΄ λλ” μ΅°μμ…λ‹λ‹¤."},
        {"role": "user", "content": "λ‘λ‘."},
    ],
)
```

ν΄λΌμ΄μ–ΈνΈλ¥Ό μ „ν™ν•  λ• ν•μ΄νΌνλΌλ―Έν„°κ°€ μ—¬μ „ν μ ν¨ν•μ§€ ν™•μΈν•μ„Έμ”. μλ¥Ό λ“¤μ–΄ GPT-3μ `temperature`λ” `Llama-2`μ `temperature`μ™€ λ‹¤λ¥Ό μ μμµλ‹λ‹¤.

## β‘οΈ μ£Όμ” κΈ°λ¥

### π¤ νΈν™λλ” ν΄λΌμ΄μ–ΈνΈ

- `openai.ChatCompletion`μ OpenAI API ν•μ‹κ³Ό νΈν™λλ” ν΄λΌμ΄μ–ΈνΈ κµ¬ν„.
- μ½”λ“ ν• μ¤„μ„ λ³€κ²½ν•μ—¬ `openai.ChatCompletion`κ³Ό `huggingface.ChatCompletion`κ³Ό κ°™μ€ λ‹¤λ¥Έ LLM κ°„μ— μ‰½κ² μ „ν™ν•  μ μμµλ‹λ‹¤.
- μ™„μ„± μ¤νΈλ¦¬λ° μ§€μ›, [μ™„μ„± μ¤νΈλ¦¬λ° λ°©λ²•](examples/stream-chat-completions) μμ  ν™•μΈ.

### β™οΈ ν—¬νΌ λ¨λ“ β™οΈ

- `evol_instruct` (μ‘μ—… μ§„ν–‰ μ¤‘) - μ§„ν™” μ•κ³ λ¦¬μ¦μ„ μ‚¬μ©ν•μ—¬ LLMμ© μ§€μΉ¨μ„ λ§λ“­λ‹λ‹¤.

- `prompt_utils` - OpenAI λ©”μ‹μ§€μ™€ κ°™μ€ ν”„λ΅¬ν”„νΈ ν•μ‹μ„ Llama 2μ™€ κ°™μ€ μ¤ν” μ†μ¤ λ¨λΈμ© ν”„λ΅¬ν”„νΈλ΅ μ‰½κ² λ³€ν™ν•λ” ν—¬νΌ λ©”μ„λ“μ…λ‹λ‹¤.

## π“” μΈμ© λ° κ°μ‚¬

EasyLLMμ„ μ‚¬μ©ν•μ‹ λ‹¤λ©΄ μ†μ… λ―Έλ””μ–΄λ‚ μ΄λ©”μΌλ΅ μ €μ™€ κ³µμ ν•΄μ£Όμ„Έμ”. μ •λ§ λ“£κ³  μ‹¶μµλ‹λ‹¤!
λ‹¤μ BibTeXμ„ μ‚¬μ©ν•μ—¬ ν”„λ΅μ νΈλ¥Ό μΈμ©ν•  μλ„ μμµλ‹λ‹¤:

```bash
@software{Philipp_Schmid_EasyLLM_2023,
author = {Philipp Schmid},
license = {Apache-2.0},
month = juj,
title = {EasyLLM: Streamlined Tools for LLMs},
url = {https://github.com/philschmid/easyllm},
year = {2023}
}
```

<!-- ## μ½”λ“

μ½”λ“μ ν•¨μ λ§ν¬:
[`κ°μ²΄ 1`][easyllm.utils.fancy_function] -->
